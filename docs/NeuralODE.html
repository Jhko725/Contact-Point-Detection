<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.7.2" />
<title>cp_detection.NeuralODE API documentation</title>
<meta name="description" content="" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>cp_detection.NeuralODE</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from __future__ import print_function, division
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint
from torchdiffeq import odeint_adjoint as odeint
import matplotlib.pyplot as plt
import json, sys, os, inspect

class GeneralModeDataset(Dataset):
    &#34;&#34;&#34;
    A PyTorch Dataset to handle general mode AFM data. 
    &#34;&#34;&#34;

    def __init__(self, t, d_array, z_array, ode_params):
        &#34;&#34;&#34;
        Parameters
        ----------
        t : 1D Numpy array 
            1D Numpy array containing the time stamps corresponding to the ODE solution x(t)
        ode_params : dict
            Dictionary containing the necessary parameters for the ODE. 
            Required form is ode_params = {&#39;A0&#39; : float, &#39;Q&#39; : float, &#39;Om&#39; : float, &#39;k&#39; : float}
        &#34;&#34;&#34;
        # Needs modifying - in the final form, we do not necessarily need x0 in both the model and the dataset
        self.t = np.array(t)
        self.d_array = np.array(d_array)
        self.z_array = np.array(z_array)
        self.ode_params = ode_params
        # Need to modify x0_array later
        self.x0_array = np.zeros((self.d_array.size, 2))
        self.x0_array[:,1] = self.d_array

    def __repr__(self):
        repr_str = &#39;A general mode dataset with ODE parameters: &#39; + &#39;, &#39;.join(&#39;{} = {}&#39;.format(k, v) for k, v in self.ode_params.items())
        return repr_str

    def __len__(self):
        return len(self.d_array)

    def __getitem__(self, idx):
        #sample = {&#39;time&#39;: self.t, &#39;d&#39;: self.d_list[idx], &#39;x0&#39;: self.x0, &#39;z&#39;: self.z_list[idx][:]}
        sample = {&#39;time&#39;: self.t, &#39;d&#39;: self.d_array[idx], &#39;x0&#39;: self.x0_array[idx][:],&#39;z&#39;: self.z_array[idx][:]}
        return sample

    def __eq__(self, other):
        &#34;&#34;&#34;
        Comparison between two GeneralModeDataset objects. True if both objects have the same ODE parameters.
        &#34;&#34;&#34;
        return self.ode_params == other.ode_params

    def AddNoise(self, SNR, seed = None):
        &#34;&#34;&#34;
        Returns a new GeneralModeDataset object with added Gaussian noise corresponding to SNR
        SNR is defined as $SNR = &lt;z^2(t)&gt;/\sigma^2$, where $noise ~ N(0, \sigma^2)$
        &#34;&#34;&#34;
        np.random.seed(seed)
        sqr_avg = np.mean(self.z_array**2, axis = -1)
        var = sqr_avg/SNR

        noise = np.stack([np.random.normal(0, v, size = self.z_array.shape[-1]) for v in var], axis = 0)
        
        noisy_dataset = GeneralModeDataset(**self._savedict())
        noisy_dataset.z_array += noise

        return noisy_dataset

    def PlotData(self, idx, ax, fontsize = 14, **kwargs):
        data = self.__getitem__(idx)
        z = data[&#39;z&#39;]
        t = data[&#39;time&#39;][-len(z):]
        ax.scatter(t, z, **kwargs)
        ax.grid(ls = &#39;--&#39;)
        ax.set_xlabel(&#39;Normalized Time&#39;, fontsize = fontsize)
        ax.set_ylabel(&#39;z (nm)&#39;, fontsize = fontsize)

        return ax

    def _savedict(self):
        &#34;&#34;&#34;
        Creates a dictionary containing only the necessary entries for the class constructor
        Directly unpacking self.__dict__ into the constructor raises errors due to class attributes originally not present in the constructor arguments
        &#34;&#34;&#34;
        savedict = {k: v for k, v in self.__dict__.items() if k in [p.name for p in inspect.signature(self.__init__).parameters.values()]}
        return savedict

    def save(self, savepath):
        &#34;&#34;&#34;
        Saves the given dataset in json format.

        Parameters
        ----------
        savepath : path
            Path to save the dataset at.
        &#34;&#34;&#34;
        savedict = self._savedict()
        for k, v in savedict.items():
            if isinstance(v, np.ndarray):
                savedict[k] = v.tolist()

        with open(savepath, &#39;w&#39;) as savefile:
            json.dump(savedict, savefile)
        print(&#39;Saved data to: {}&#39;.format(savepath))

    @classmethod
    def load(cls, loadpath):
        &#34;&#34;&#34;
        Loads the dataset from a json file.

        Parameters
        ----------
        loadpath : path
            Path to the json file to be loaded.
        
        Returns
        -------
        dataset : An instance of the class GeneralModeDataset 
            Loaded dataset.
        &#34;&#34;&#34;
        with open(loadpath) as loadfile:
            data_dict = json.load(loadfile)
        
        return cls(**data_dict)

class F_cons(nn.Module):
    &#34;&#34;&#34;
    A PyTorch module to model the conservative force experienced by the atomic force microscope probe.
    We assume that the force only depends on z, and model it using a simple MLP.

    ...

    Attributes
    ----------
    hidden_nodes : list of int
        List of the nodes in each of the hidden layers of the model
    layers : list of torch.nn.Linear objects
        List of layers in the model. All the layers used are fully connected layers.
    elu : torch.nn.ELU object
        Elu activation layer, used for the activations between the hidden layers.
    tanh : torch.nn.Tanh object
        Tanh activation layer, used for the actvation for the model output.

    Methods
    -------
    forward(z)
        Returns the neural network output as a function of input z.
        z is assumed to be a tensor with size [1].
    &#34;&#34;&#34;

    def __init__(self, hidden_nodes = [4]):
        &#34;&#34;&#34;
        Parameters
        ----------
        hidden_nodes : list of int
            List of the nodes in each of the hidden layers of the model
        &#34;&#34;&#34;
        super(F_cons, self).__init__()
        self.hidden_nodes = np.array(hidden_nodes, dtype = int)
        self.layers = nn.ModuleList()
        for i in range(len(self.hidden_nodes)):
            if i == 0:
                self.layers.append(nn.Linear(1, self.hidden_nodes[i]))
            else:
                self.layers.append(nn.Linear(self.hidden_nodes[i-1], self.hidden_nodes[i]))
        self.layers.append(nn.Linear(self.hidden_nodes[-1], 1))

        self.elu = nn.ELU()
        self.tanh = nn.Tanh()

        # Initialize weights and biases
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, mean=0, std=1.0e-5)
                nn.init.constant_(m.bias, val=0)

    def forward(self, z):
        &#34;&#34;&#34;
        Returns the neural network output as a function of input z.
        z is assumed to be a tensor with size [batch_size, 1].

        Parameters
        ----------
        z : tensor with dimensions [batch_size, 1].
            Neural network input. Represents the instantaneous tip-sample distance.

        Returns
        -------
        F : tensor with dimensions [batch_size, 1].
            Neural network output. Represents the modeled tip-sample force.
        &#34;&#34;&#34;
        interm = self.layers[0](z)
        
        for layer in self.layers[1:]:
            interm = self.tanh(interm)
            interm = layer(interm)

        #F = self.tanh(interm)
        return interm

class AFM_NeuralODE(nn.Module):
    &#34;&#34;&#34;
    A  Pytorch module to create a NeuralODE modeling the AFM tip-sample dynamics.
    Note that all length scales involved in the model are scaled by setting 1[nm] = 1 
    and all timescales scaled to w0*1[s] = 1.

    ...

    Attributes
    ----------
    Fc : An instance of class F_cons
        A MLP model to represent the conservative force between the tip and the sample.
    nfe : int
        Number of forward evaluations. Incremented by 1 everytime forward() is evaluated.
    d : float
        Mean tip-sample distance in units of [nm].
    A0 : float
        Free amplitude of the tip at resonance in units of [nm].
        Follows the definition outlined in M. Lee et. al., Phys. Rev. Lett. 97, 036104 (2006).
    Om : float
        Ratio between the resonance frequency f0 and the driving frequency f. Om = f/f0
    Q : float
        Q-factor of the cantilever/QTF.
    k : float
        Spring constant of the cantilever/QTF in units of [N/m].

    Methods
    -------
    forward(t, x)
        Returns the right-hand-side of the differential equation dx/dt = f(t, x)
        x is a length 2 vector of the form x = [y, z], where y = dz/dt 
    &#34;&#34;&#34;

    def __init__(self, A0, Om, Q, k, hidden_nodes = [4]):
        &#34;&#34;&#34;
        Parameters
        ----------
        hidden_nodes : list of int
            List of the nodes in each of the hidden layers of the model.
        A0 : float
            Free amplitude of the tip at resonance in units of [nm].
            Follows the definition outlined in M. Lee et. al., Phys. Rev. Lett. 97, 036104 (2006).
        Om : float
            Ratio between the resonance frequency f0 and the driving frequency f. Om = f/f0
        Q : float
            Q-factor of the cantilever/QTF.
        k : float
            Spring constant of the cantilever/QTF in units of [N/m].
        d : PyTorch tensor with dimensions [batch_size, 1]
            Batched mean tip-sample distance in units of [nm]. Initialized to None.
        &#34;&#34;&#34;
        super(AFM_NeuralODE, self).__init__()
        self.Fc = F_cons(hidden_nodes)
        self.nfe = 0

        self.d = None
        self.A0 = A0
        self.Om = Om
        self.Q = Q
        self.k = k

        # Constant tensors to be used in the model
        self.C1 = torch.tensor([[-1./self.Q, -1.], [1., 0.]], device = torch.device(&#34;cuda&#34;)).unsqueeze(0) # Has size = (1, 2, 2)
        self.C2 = torch.tensor([[1.],[0.]], device = torch.device(&#34;cuda&#34;)).unsqueeze(0) # Has size = (1, 2, 1)
        self.register_buffer(&#39;Constant 1&#39;, self.C1)
        self.register_buffer(&#39;Constant 2&#39;, self.C2)

    def forward(self, t, x):
        &#34;&#34;&#34;
        Returns the right-hand-side of the differential equation dx/dt = f(t, x)
        x is a PyTorch tensor with size [batch_size, 2], where the second dimension corresponds to length 2 vector of the x = [y, z], where y = dz/dt 

        Parameters
        ----------
        t : float
            Time
        x : PyTorch tensor with dimensions [batch_size, 2]
            The second dimension correspond to x = [y, z], where y = dz/dt

        Returns
        -------
        dxdt : PyTorch tensor with dimensions [batch_size, 2]
            The second dimension corresponds to dxdt = [dy/dt, dz/dt]
        &#34;&#34;&#34;
        self.nfe += 1
        F = self.Fc(x[:, 1:])
    
        # The first term is broadcasted matrix multiplication of (1, 2, 2) * (b, 2, 1) = (b, 2, 1), where b = self.batch_size.
        # The second term is broadcasted matrix multiplication of (1, 2, 1) * (b, 1, 1) = (b, 2, 1)
        ode = torch.matmul(self.C1, x.unsqueeze(-1)) + torch.matmul(self.C2, (self.d + (self.A0/self.Q)*torch.cos(self.Om*t) + F/self.k).unsqueeze(-1))

        # Squeeze to return a tensor of shape (b, 2)
        return ode.squeeze(-1)


class LightningTrainer(pl.LightningModule):
    &#34;&#34;&#34;
    A PyTorch-Lightning LightningModule for training the NeuralODE created by the class AFM_NeuralODE. 

    ...

    Attributes
    ----------
    ODE : An instance of class AFM_NeuralODE
        A NeuralODE model to represent the dynamics between the tip and the sample.
    hparams : An argparse.Namespace object with fields &#39;train_dataset&#39;, &#39;hidden_nodes&#39;, &#39;batch_size&#39;, &#39;lr&#39;, and &#39;solver&#39;.
            Hyperparameters of the model. 
            &#39;train_dataset&#39; : An instance of the class GeneralModeDataset
                A PyTorch dataset of a given general mode approach data to train the NeuralODE with. 
            &#39;hidden nodes&#39; : list or array of integers
                List of number of nodes in the hidden layers of the model.
            &#39;batch_size&#39; : integer
                Batch_size of the model
            &#39;lr&#39; : float
                Learning rate
            &#39;solver&#39; : string, must be compatible with TorchDiffEq.odeint_adjoint
                Type of ODE solver to be used to solve the NeuralODE
    &#34;&#34;&#34;

    #def __init__(self, train_dataset, lr = 0.05, hidden_nodes = [10, 10], batch_size = 1, solver = &#39;dopri5&#39;):
    def __init__(self, hparams, verbose = True):
        &#34;&#34;&#34;
        Parameters
        ----------
        hparams : An argparse.Namespace object with fields &#39;train_dataset&#39;, &#39;hidden_nodes&#39;, &#39;batch_size&#39;, &#39;lr&#39;, and &#39;solver&#39;.
            Hyperparameters of the model. 
            &#39;train_dataset&#39; : An instance of the class GeneralModeDataset
                A PyTorch dataset of a given general mode approach data to train the NeuralODE with. 
            &#39;hidden nodes&#39; : list or array of integers
                List of number of nodes in the hidden layers of the model.
            &#39;batch_size&#39; : integer
                Batch_size of the model
            &#39;lr&#39; : float
                Learning rate
            &#39;solver&#39; : string, must be compatible with TorchDiffEq.odeint_adjoint
                Type of ODE solver to be used to solve the NeuralODE
        &#34;&#34;&#34;
        super(LightningTrainer, self).__init__()
        self.hparams = hparams
        self.train_dataset = GeneralModeDataset.load(self.hparams.train_dataset_path)
        ode_params = self.train_dataset.ode_params
        self.ODE = AFM_NeuralODE(**ode_params, hidden_nodes = self.hparams.hidden_nodes)
        self.batch_size = self.hparams.batch_size
        self.lr = self.hparams.lr
        self.solver = self.hparams.solver
        self._verbose = verbose
        self._fft_loss = self.hparams.fft_loss

    def forward(self, t, x0, d):
        self.ODE.d = d.view(self.batch_size, 1)

        x_pred = odeint(self.ODE, x0, t, method = self.solver, rtol = 1e-6)
        # x_pred has shape = [time, batch_size, 2]. Permute z_pred so that it has shape = [batch_size, time]
        z_pred = x_pred[:,:,1].permute(1,0)

        return z_pred

    def training_step(self, batch, batch_nb):
        t = batch[&#39;time&#39;][0].float()
        x0 = batch[&#39;x0&#39;].float()
        d = batch[&#39;d&#39;].float()

        z_true = batch[&#39;z&#39;].float()
        N_data = z_true.size(1) # length of the validation data
        
        if self._verbose:
            sys.stdout.write(&#39;\r&#39;)
            sys.stdout.write(&#39;Forward-propagating...&#39;)
            sys.stdout.flush()

        z_pred = self.forward(t, x0, d)[:, -N_data:]
        loss_function = nn.MSELoss()
        if self._fft_loss:
            log1pI_pred = self.LogSpectra(z_pred)
            log1pI_true = self.LogSpectra(z_true)

            loss = loss_function(log1pI_pred, log1pI_true)
        else:
            loss = loss_function(z_true, z_pred)

        if self._verbose:
            sys.stdout.write(&#39;\r&#39;)
            sys.stdout.write(&#39;Backward-propagating...&#39;)
            sys.stdout.flush()

        tensorboard_logs = {&#39;train_loss&#39;: loss}
        return {&#39;loss&#39;: loss, &#39;log&#39;: tensorboard_logs}

    def train_dataloader(self):
        return DataLoader(self.train_dataset, batch_size = self.batch_size, shuffle = True)

    def validation_step(self, batch, batch_nb):
        pass

    @staticmethod
    def LogSpectra(z):
        &#34;&#34;&#34;
        Calculates the complex FFT spectra of the given signal z(t), then calculates the intensity. Finally, returns log1p(intensity), where log1p is preferred over log due to its numerical stability.

        Parameters
        ----------
        z : A 1D PyTorch tensor
            1D tensor of real values, corresponding to the time series z(t)

        Returns
        -------
        z_log1pI : A 1d Pytorch tensor
            1D tensor corresponding to the log1p of the Fourier intensity of z(t).
        &#34;&#34;&#34;

        z_fft = torch.rfft(z, 1)
        #z_I = torch.sum(z_fft**2, dim = -1)
        #z_log1pI = torch.norm(z_fft, p = 2, dim = -1)
        #z_log1pI = torch.log1p(z_I)

        return z_fft

    def configure_optimizers(self):
        optim = torch.optim.Adam(self.parameters(), lr = self.lr)
        #sched = torch.optim.lr_scheduler.ReduceLROnPlateau(optim, &#39;min&#39;, 0.5, 100, threshold = 0.05, threshold_mode = &#39;rel&#39;)
        #return [optim], [sched]
        return optim

    def predict_z(self):
        pass

    def predict_force(self, d):
        d = np.array(d)
        d_tensor = torch.from_numpy(d).cuda(non_blocking = True).float().reshape(-1, 1)
        F_pred = self.ODE.Fc(d_tensor).cpu().detach().numpy()
        return F_pred

    def TrainModel(self, max_epochs = 10000, checkpoint_path = &#39;./checkpoints&#39;):
        &#34;&#34;&#34;
        Trains the model using PyTorch_Lightning.trainer. 

        Parameters
        ----------
        model : An instance of PyTorch_Lightning.LightningModule
            Neural network to be trained. 
        max_epochs : integer
            Maximum number of training.
        checkpoint_path : path
            Path to save the checkpointed model during training.
        &#34;&#34;&#34;
        #logger = TensorBoardLogger(save_dir = os.getcwd(), version = self.slurm_job_id, name = &#39;lightning_logs&#39;)
        checkpoint_callback = ModelCheckpoint(filepath = checkpoint_path, save_top_k = 1, verbose = True, monitor = &#39;loss&#39;, mode = &#39;min&#39;)
        trainer = pl.Trainer(gpus = 1, checkpoint_callback = checkpoint_callback, early_stop_callback = None, show_progress_bar = True, max_nb_epochs = max_epochs, log_save_interval = 1)
        trainer.fit(self)

    @classmethod
    def LoadModel(cls, checkpoint_path):
        &#34;&#34;&#34;
        Loads the checkpointed model from checkpoint_path. 

        Parameters
        ----------
        checkpoint_path : path
            Path to load the checkpointed model from.

        Returns
        -------
        loaded_model : An instance of PyTorch_Lightning.LightningModule
            Loaded neural network.
        &#34;&#34;&#34;
        return cls.load_from_checkpoint(checkpoint_path)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="cp_detection.NeuralODE.AFM_NeuralODE"><code class="flex name class">
<span>class <span class="ident">AFM_NeuralODE</span></span>
<span>(</span><span>A0, Om, Q, k, hidden_nodes=[4])</span>
</code></dt>
<dd>
<section class="desc"><p>A
Pytorch module to create a NeuralODE modeling the AFM tip-sample dynamics.
Note that all length scales involved in the model are scaled by setting 1[nm] = 1
and all timescales scaled to w0*1[s] = 1.</p>
<p>&hellip;</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>Fc</code></strong> :&ensp;<code>An</code> <code>instance</code> of <code>class</code> <a title="cp_detection.NeuralODE.F_cons" href="#cp_detection.NeuralODE.F_cons"><code>F_cons</code></a></dt>
<dd>A MLP model to represent the conservative force between the tip and the sample.</dd>
<dt><strong><code>nfe</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of forward evaluations. Incremented by 1 everytime forward() is evaluated.</dd>
<dt><strong><code>d</code></strong> :&ensp;<code>float</code></dt>
<dd>Mean tip-sample distance in units of [nm].</dd>
<dt><strong><code>A0</code></strong> :&ensp;<code>float</code></dt>
<dd>Free amplitude of the tip at resonance in units of [nm].
Follows the definition outlined in M. Lee et. al., Phys. Rev. Lett. 97, 036104 (2006).</dd>
<dt><strong><code>Om</code></strong> :&ensp;<code>float</code></dt>
<dd>Ratio between the resonance frequency f0 and the driving frequency f. Om = f/f0</dd>
<dt><strong><code>Q</code></strong> :&ensp;<code>float</code></dt>
<dd>Q-factor of the cantilever/QTF.</dd>
<dt><strong><code>k</code></strong> :&ensp;<code>float</code></dt>
<dd>Spring constant of the cantilever/QTF in units of [N/m].</dd>
</dl>
<h2 id="methods">Methods</h2>
<p>forward(t, x)
Returns the right-hand-side of the differential equation dx/dt = f(t, x)
x is a length 2 vector of the form x = [y, z], where y = dz/dt </p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>hidden_nodes</code></strong> :&ensp;<code>list</code> of <code>int</code></dt>
<dd>List of the nodes in each of the hidden layers of the model.</dd>
<dt><strong><code>A0</code></strong> :&ensp;<code>float</code></dt>
<dd>Free amplitude of the tip at resonance in units of [nm].
Follows the definition outlined in M. Lee et. al., Phys. Rev. Lett. 97, 036104 (2006).</dd>
<dt><strong><code>Om</code></strong> :&ensp;<code>float</code></dt>
<dd>Ratio between the resonance frequency f0 and the driving frequency f. Om = f/f0</dd>
<dt><strong><code>Q</code></strong> :&ensp;<code>float</code></dt>
<dd>Q-factor of the cantilever/QTF.</dd>
<dt><strong><code>k</code></strong> :&ensp;<code>float</code></dt>
<dd>Spring constant of the cantilever/QTF in units of [N/m].</dd>
<dt><strong><code>d</code></strong> :&ensp;<code>PyTorch</code> <code>tensor</code> <code>with</code> <code>dimensions</code> [<code>batch_size</code>, <code>1</code>]</dt>
<dd>Batched mean tip-sample distance in units of [nm]. Initialized to None.</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AFM_NeuralODE(nn.Module):
    &#34;&#34;&#34;
    A  Pytorch module to create a NeuralODE modeling the AFM tip-sample dynamics.
    Note that all length scales involved in the model are scaled by setting 1[nm] = 1 
    and all timescales scaled to w0*1[s] = 1.

    ...

    Attributes
    ----------
    Fc : An instance of class F_cons
        A MLP model to represent the conservative force between the tip and the sample.
    nfe : int
        Number of forward evaluations. Incremented by 1 everytime forward() is evaluated.
    d : float
        Mean tip-sample distance in units of [nm].
    A0 : float
        Free amplitude of the tip at resonance in units of [nm].
        Follows the definition outlined in M. Lee et. al., Phys. Rev. Lett. 97, 036104 (2006).
    Om : float
        Ratio between the resonance frequency f0 and the driving frequency f. Om = f/f0
    Q : float
        Q-factor of the cantilever/QTF.
    k : float
        Spring constant of the cantilever/QTF in units of [N/m].

    Methods
    -------
    forward(t, x)
        Returns the right-hand-side of the differential equation dx/dt = f(t, x)
        x is a length 2 vector of the form x = [y, z], where y = dz/dt 
    &#34;&#34;&#34;

    def __init__(self, A0, Om, Q, k, hidden_nodes = [4]):
        &#34;&#34;&#34;
        Parameters
        ----------
        hidden_nodes : list of int
            List of the nodes in each of the hidden layers of the model.
        A0 : float
            Free amplitude of the tip at resonance in units of [nm].
            Follows the definition outlined in M. Lee et. al., Phys. Rev. Lett. 97, 036104 (2006).
        Om : float
            Ratio between the resonance frequency f0 and the driving frequency f. Om = f/f0
        Q : float
            Q-factor of the cantilever/QTF.
        k : float
            Spring constant of the cantilever/QTF in units of [N/m].
        d : PyTorch tensor with dimensions [batch_size, 1]
            Batched mean tip-sample distance in units of [nm]. Initialized to None.
        &#34;&#34;&#34;
        super(AFM_NeuralODE, self).__init__()
        self.Fc = F_cons(hidden_nodes)
        self.nfe = 0

        self.d = None
        self.A0 = A0
        self.Om = Om
        self.Q = Q
        self.k = k

        # Constant tensors to be used in the model
        self.C1 = torch.tensor([[-1./self.Q, -1.], [1., 0.]], device = torch.device(&#34;cuda&#34;)).unsqueeze(0) # Has size = (1, 2, 2)
        self.C2 = torch.tensor([[1.],[0.]], device = torch.device(&#34;cuda&#34;)).unsqueeze(0) # Has size = (1, 2, 1)
        self.register_buffer(&#39;Constant 1&#39;, self.C1)
        self.register_buffer(&#39;Constant 2&#39;, self.C2)

    def forward(self, t, x):
        &#34;&#34;&#34;
        Returns the right-hand-side of the differential equation dx/dt = f(t, x)
        x is a PyTorch tensor with size [batch_size, 2], where the second dimension corresponds to length 2 vector of the x = [y, z], where y = dz/dt 

        Parameters
        ----------
        t : float
            Time
        x : PyTorch tensor with dimensions [batch_size, 2]
            The second dimension correspond to x = [y, z], where y = dz/dt

        Returns
        -------
        dxdt : PyTorch tensor with dimensions [batch_size, 2]
            The second dimension corresponds to dxdt = [dy/dt, dz/dt]
        &#34;&#34;&#34;
        self.nfe += 1
        F = self.Fc(x[:, 1:])
    
        # The first term is broadcasted matrix multiplication of (1, 2, 2) * (b, 2, 1) = (b, 2, 1), where b = self.batch_size.
        # The second term is broadcasted matrix multiplication of (1, 2, 1) * (b, 1, 1) = (b, 2, 1)
        ode = torch.matmul(self.C1, x.unsqueeze(-1)) + torch.matmul(self.C2, (self.d + (self.A0/self.Q)*torch.cos(self.Om*t) + F/self.k).unsqueeze(-1))

        # Squeeze to return a tensor of shape (b, 2)
        return ode.squeeze(-1)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="cp_detection.NeuralODE.AFM_NeuralODE.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, t, x)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the right-hand-side of the differential equation dx/dt = f(t, x)
x is a PyTorch tensor with size [batch_size, 2], where the second dimension corresponds to length 2 vector of the x = [y, z], where y = dz/dt </p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>t</code></strong> :&ensp;<code>float</code></dt>
<dd>Time</dd>
<dt><strong><code>x</code></strong> :&ensp;<code>PyTorch</code> <code>tensor</code> <code>with</code> <code>dimensions</code> [<code>batch_size</code>, <code>2</code>]</dt>
<dd>The second dimension correspond to x = [y, z], where y = dz/dt</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dxdt</code></strong> :&ensp;<code>PyTorch</code> <code>tensor</code> <code>with</code> <code>dimensions</code> [<code>batch_size</code>, <code>2</code>]</dt>
<dd>The second dimension corresponds to dxdt = [dy/dt, dz/dt]</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, t, x):
    &#34;&#34;&#34;
    Returns the right-hand-side of the differential equation dx/dt = f(t, x)
    x is a PyTorch tensor with size [batch_size, 2], where the second dimension corresponds to length 2 vector of the x = [y, z], where y = dz/dt 

    Parameters
    ----------
    t : float
        Time
    x : PyTorch tensor with dimensions [batch_size, 2]
        The second dimension correspond to x = [y, z], where y = dz/dt

    Returns
    -------
    dxdt : PyTorch tensor with dimensions [batch_size, 2]
        The second dimension corresponds to dxdt = [dy/dt, dz/dt]
    &#34;&#34;&#34;
    self.nfe += 1
    F = self.Fc(x[:, 1:])

    # The first term is broadcasted matrix multiplication of (1, 2, 2) * (b, 2, 1) = (b, 2, 1), where b = self.batch_size.
    # The second term is broadcasted matrix multiplication of (1, 2, 1) * (b, 1, 1) = (b, 2, 1)
    ode = torch.matmul(self.C1, x.unsqueeze(-1)) + torch.matmul(self.C2, (self.d + (self.A0/self.Q)*torch.cos(self.Om*t) + F/self.k).unsqueeze(-1))

    # Squeeze to return a tensor of shape (b, 2)
    return ode.squeeze(-1)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="cp_detection.NeuralODE.F_cons"><code class="flex name class">
<span>class <span class="ident">F_cons</span></span>
<span>(</span><span>hidden_nodes=[4])</span>
</code></dt>
<dd>
<section class="desc"><p>A PyTorch module to model the conservative force experienced by the atomic force microscope probe.
We assume that the force only depends on z, and model it using a simple MLP.</p>
<p>&hellip;</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>hidden_nodes</code></strong> :&ensp;<code>list</code> of <code>int</code></dt>
<dd>List of the nodes in each of the hidden layers of the model</dd>
<dt><strong><code>layers</code></strong> :&ensp;<code>list</code> of <code>torch.nn.Linear</code> <code>objects</code></dt>
<dd>List of layers in the model. All the layers used are fully connected layers.</dd>
<dt><strong><code>elu</code></strong> :&ensp;<code>torch.nn.ELU</code> <code>object</code></dt>
<dd>Elu activation layer, used for the activations between the hidden layers.</dd>
<dt><strong><code>tanh</code></strong> :&ensp;<code>torch.nn.Tanh</code> <code>object</code></dt>
<dd>Tanh activation layer, used for the actvation for the model output.</dd>
</dl>
<h2 id="methods">Methods</h2>
<p>forward(z)
Returns the neural network output as a function of input z.
z is assumed to be a tensor with size [1].</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>hidden_nodes</code></strong> :&ensp;<code>list</code> of <code>int</code></dt>
<dd>List of the nodes in each of the hidden layers of the model</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class F_cons(nn.Module):
    &#34;&#34;&#34;
    A PyTorch module to model the conservative force experienced by the atomic force microscope probe.
    We assume that the force only depends on z, and model it using a simple MLP.

    ...

    Attributes
    ----------
    hidden_nodes : list of int
        List of the nodes in each of the hidden layers of the model
    layers : list of torch.nn.Linear objects
        List of layers in the model. All the layers used are fully connected layers.
    elu : torch.nn.ELU object
        Elu activation layer, used for the activations between the hidden layers.
    tanh : torch.nn.Tanh object
        Tanh activation layer, used for the actvation for the model output.

    Methods
    -------
    forward(z)
        Returns the neural network output as a function of input z.
        z is assumed to be a tensor with size [1].
    &#34;&#34;&#34;

    def __init__(self, hidden_nodes = [4]):
        &#34;&#34;&#34;
        Parameters
        ----------
        hidden_nodes : list of int
            List of the nodes in each of the hidden layers of the model
        &#34;&#34;&#34;
        super(F_cons, self).__init__()
        self.hidden_nodes = np.array(hidden_nodes, dtype = int)
        self.layers = nn.ModuleList()
        for i in range(len(self.hidden_nodes)):
            if i == 0:
                self.layers.append(nn.Linear(1, self.hidden_nodes[i]))
            else:
                self.layers.append(nn.Linear(self.hidden_nodes[i-1], self.hidden_nodes[i]))
        self.layers.append(nn.Linear(self.hidden_nodes[-1], 1))

        self.elu = nn.ELU()
        self.tanh = nn.Tanh()

        # Initialize weights and biases
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, mean=0, std=1.0e-5)
                nn.init.constant_(m.bias, val=0)

    def forward(self, z):
        &#34;&#34;&#34;
        Returns the neural network output as a function of input z.
        z is assumed to be a tensor with size [batch_size, 1].

        Parameters
        ----------
        z : tensor with dimensions [batch_size, 1].
            Neural network input. Represents the instantaneous tip-sample distance.

        Returns
        -------
        F : tensor with dimensions [batch_size, 1].
            Neural network output. Represents the modeled tip-sample force.
        &#34;&#34;&#34;
        interm = self.layers[0](z)
        
        for layer in self.layers[1:]:
            interm = self.tanh(interm)
            interm = layer(interm)

        #F = self.tanh(interm)
        return interm</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="cp_detection.NeuralODE.F_cons.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, z)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the neural network output as a function of input z.
z is assumed to be a tensor with size [batch_size, 1].</p>
<h2 id="parameters">Parameters</h2>
<p>z : tensor with dimensions [batch_size, 1].
Neural network input. Represents the instantaneous tip-sample distance.</p>
<h2 id="returns">Returns</h2>
<p>F : tensor with dimensions [batch_size, 1].
Neural network output. Represents the modeled tip-sample force.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, z):
    &#34;&#34;&#34;
    Returns the neural network output as a function of input z.
    z is assumed to be a tensor with size [batch_size, 1].

    Parameters
    ----------
    z : tensor with dimensions [batch_size, 1].
        Neural network input. Represents the instantaneous tip-sample distance.

    Returns
    -------
    F : tensor with dimensions [batch_size, 1].
        Neural network output. Represents the modeled tip-sample force.
    &#34;&#34;&#34;
    interm = self.layers[0](z)
    
    for layer in self.layers[1:]:
        interm = self.tanh(interm)
        interm = layer(interm)

    #F = self.tanh(interm)
    return interm</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="cp_detection.NeuralODE.GeneralModeDataset"><code class="flex name class">
<span>class <span class="ident">GeneralModeDataset</span></span>
<span>(</span><span>t, d_array, z_array, ode_params)</span>
</code></dt>
<dd>
<section class="desc"><p>A PyTorch Dataset to handle general mode AFM data. </p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>t</code></strong> :&ensp;<code>1D</code> <code>Numpy</code> <code>array</code></dt>
<dd>1D Numpy array containing the time stamps corresponding to the ODE solution x(t)</dd>
<dt><strong><code>ode_params</code></strong> :&ensp;<code>dict</code></dt>
<dd>Dictionary containing the necessary parameters for the ODE.
Required form is ode_params = {'A0' : float, 'Q' : float, 'Om' : float, 'k' : float}</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class GeneralModeDataset(Dataset):
    &#34;&#34;&#34;
    A PyTorch Dataset to handle general mode AFM data. 
    &#34;&#34;&#34;

    def __init__(self, t, d_array, z_array, ode_params):
        &#34;&#34;&#34;
        Parameters
        ----------
        t : 1D Numpy array 
            1D Numpy array containing the time stamps corresponding to the ODE solution x(t)
        ode_params : dict
            Dictionary containing the necessary parameters for the ODE. 
            Required form is ode_params = {&#39;A0&#39; : float, &#39;Q&#39; : float, &#39;Om&#39; : float, &#39;k&#39; : float}
        &#34;&#34;&#34;
        # Needs modifying - in the final form, we do not necessarily need x0 in both the model and the dataset
        self.t = np.array(t)
        self.d_array = np.array(d_array)
        self.z_array = np.array(z_array)
        self.ode_params = ode_params
        # Need to modify x0_array later
        self.x0_array = np.zeros((self.d_array.size, 2))
        self.x0_array[:,1] = self.d_array

    def __repr__(self):
        repr_str = &#39;A general mode dataset with ODE parameters: &#39; + &#39;, &#39;.join(&#39;{} = {}&#39;.format(k, v) for k, v in self.ode_params.items())
        return repr_str

    def __len__(self):
        return len(self.d_array)

    def __getitem__(self, idx):
        #sample = {&#39;time&#39;: self.t, &#39;d&#39;: self.d_list[idx], &#39;x0&#39;: self.x0, &#39;z&#39;: self.z_list[idx][:]}
        sample = {&#39;time&#39;: self.t, &#39;d&#39;: self.d_array[idx], &#39;x0&#39;: self.x0_array[idx][:],&#39;z&#39;: self.z_array[idx][:]}
        return sample

    def __eq__(self, other):
        &#34;&#34;&#34;
        Comparison between two GeneralModeDataset objects. True if both objects have the same ODE parameters.
        &#34;&#34;&#34;
        return self.ode_params == other.ode_params

    def AddNoise(self, SNR, seed = None):
        &#34;&#34;&#34;
        Returns a new GeneralModeDataset object with added Gaussian noise corresponding to SNR
        SNR is defined as $SNR = &lt;z^2(t)&gt;/\sigma^2$, where $noise ~ N(0, \sigma^2)$
        &#34;&#34;&#34;
        np.random.seed(seed)
        sqr_avg = np.mean(self.z_array**2, axis = -1)
        var = sqr_avg/SNR

        noise = np.stack([np.random.normal(0, v, size = self.z_array.shape[-1]) for v in var], axis = 0)
        
        noisy_dataset = GeneralModeDataset(**self._savedict())
        noisy_dataset.z_array += noise

        return noisy_dataset

    def PlotData(self, idx, ax, fontsize = 14, **kwargs):
        data = self.__getitem__(idx)
        z = data[&#39;z&#39;]
        t = data[&#39;time&#39;][-len(z):]
        ax.scatter(t, z, **kwargs)
        ax.grid(ls = &#39;--&#39;)
        ax.set_xlabel(&#39;Normalized Time&#39;, fontsize = fontsize)
        ax.set_ylabel(&#39;z (nm)&#39;, fontsize = fontsize)

        return ax

    def _savedict(self):
        &#34;&#34;&#34;
        Creates a dictionary containing only the necessary entries for the class constructor
        Directly unpacking self.__dict__ into the constructor raises errors due to class attributes originally not present in the constructor arguments
        &#34;&#34;&#34;
        savedict = {k: v for k, v in self.__dict__.items() if k in [p.name for p in inspect.signature(self.__init__).parameters.values()]}
        return savedict

    def save(self, savepath):
        &#34;&#34;&#34;
        Saves the given dataset in json format.

        Parameters
        ----------
        savepath : path
            Path to save the dataset at.
        &#34;&#34;&#34;
        savedict = self._savedict()
        for k, v in savedict.items():
            if isinstance(v, np.ndarray):
                savedict[k] = v.tolist()

        with open(savepath, &#39;w&#39;) as savefile:
            json.dump(savedict, savefile)
        print(&#39;Saved data to: {}&#39;.format(savepath))

    @classmethod
    def load(cls, loadpath):
        &#34;&#34;&#34;
        Loads the dataset from a json file.

        Parameters
        ----------
        loadpath : path
            Path to the json file to be loaded.
        
        Returns
        -------
        dataset : An instance of the class GeneralModeDataset 
            Loaded dataset.
        &#34;&#34;&#34;
        with open(loadpath) as loadfile:
            data_dict = json.load(loadfile)
        
        return cls(**data_dict)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.utils.data.dataset.Dataset</li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="cp_detection.NeuralODE.GeneralModeDataset.load"><code class="name flex">
<span>def <span class="ident">load</span></span>(<span>loadpath)</span>
</code></dt>
<dd>
<section class="desc"><p>Loads the dataset from a json file.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>loadpath</code></strong> :&ensp;<code>path</code></dt>
<dd>Path to the json file to be loaded.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dataset</code></strong> :&ensp;<code>An</code> <code>instance</code> of <code>the</code> <code>class</code> <a title="cp_detection.NeuralODE.GeneralModeDataset" href="#cp_detection.NeuralODE.GeneralModeDataset"><code>GeneralModeDataset</code></a></dt>
<dd>Loaded dataset.</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def load(cls, loadpath):
    &#34;&#34;&#34;
    Loads the dataset from a json file.

    Parameters
    ----------
    loadpath : path
        Path to the json file to be loaded.
    
    Returns
    -------
    dataset : An instance of the class GeneralModeDataset 
        Loaded dataset.
    &#34;&#34;&#34;
    with open(loadpath) as loadfile:
        data_dict = json.load(loadfile)
    
    return cls(**data_dict)</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="cp_detection.NeuralODE.GeneralModeDataset.AddNoise"><code class="name flex">
<span>def <span class="ident">AddNoise</span></span>(<span>self, SNR, seed=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns a new GeneralModeDataset object with added Gaussian noise corresponding to SNR
SNR is defined as $SNR = <z^2(t)>/\sigma^2$, where $noise ~ N(0, \sigma^2)$</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def AddNoise(self, SNR, seed = None):
    &#34;&#34;&#34;
    Returns a new GeneralModeDataset object with added Gaussian noise corresponding to SNR
    SNR is defined as $SNR = &lt;z^2(t)&gt;/\sigma^2$, where $noise ~ N(0, \sigma^2)$
    &#34;&#34;&#34;
    np.random.seed(seed)
    sqr_avg = np.mean(self.z_array**2, axis = -1)
    var = sqr_avg/SNR

    noise = np.stack([np.random.normal(0, v, size = self.z_array.shape[-1]) for v in var], axis = 0)
    
    noisy_dataset = GeneralModeDataset(**self._savedict())
    noisy_dataset.z_array += noise

    return noisy_dataset</code></pre>
</details>
</dd>
<dt id="cp_detection.NeuralODE.GeneralModeDataset.PlotData"><code class="name flex">
<span>def <span class="ident">PlotData</span></span>(<span>self, idx, ax, fontsize=14, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def PlotData(self, idx, ax, fontsize = 14, **kwargs):
    data = self.__getitem__(idx)
    z = data[&#39;z&#39;]
    t = data[&#39;time&#39;][-len(z):]
    ax.scatter(t, z, **kwargs)
    ax.grid(ls = &#39;--&#39;)
    ax.set_xlabel(&#39;Normalized Time&#39;, fontsize = fontsize)
    ax.set_ylabel(&#39;z (nm)&#39;, fontsize = fontsize)

    return ax</code></pre>
</details>
</dd>
<dt id="cp_detection.NeuralODE.GeneralModeDataset.save"><code class="name flex">
<span>def <span class="ident">save</span></span>(<span>self, savepath)</span>
</code></dt>
<dd>
<section class="desc"><p>Saves the given dataset in json format.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>savepath</code></strong> :&ensp;<code>path</code></dt>
<dd>Path to save the dataset at.</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save(self, savepath):
    &#34;&#34;&#34;
    Saves the given dataset in json format.

    Parameters
    ----------
    savepath : path
        Path to save the dataset at.
    &#34;&#34;&#34;
    savedict = self._savedict()
    for k, v in savedict.items():
        if isinstance(v, np.ndarray):
            savedict[k] = v.tolist()

    with open(savepath, &#39;w&#39;) as savefile:
        json.dump(savedict, savefile)
    print(&#39;Saved data to: {}&#39;.format(savepath))</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="cp_detection.NeuralODE.LightningTrainer"><code class="flex name class">
<span>class <span class="ident">LightningTrainer</span></span>
<span>(</span><span>hparams, verbose=True)</span>
</code></dt>
<dd>
<section class="desc"><p>A PyTorch-Lightning LightningModule for training the NeuralODE created by the class AFM_NeuralODE. </p>
<p>&hellip;</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>ODE</code></strong> :&ensp;<code>An</code> <code>instance</code> of <code>class</code> <a title="cp_detection.NeuralODE.AFM_NeuralODE" href="#cp_detection.NeuralODE.AFM_NeuralODE"><code>AFM_NeuralODE</code></a></dt>
<dd>A NeuralODE model to represent the dynamics between the tip and the sample.</dd>
</dl>
<p>hparams : An argparse.Namespace object with fields 'train_dataset', 'hidden_nodes', 'batch_size', 'lr', and 'solver'.
Hyperparameters of the model.
'train_dataset' : An instance of the class GeneralModeDataset
A PyTorch dataset of a given general mode approach data to train the NeuralODE with.
'hidden nodes' : list or array of integers
List of number of nodes in the hidden layers of the model.
'batch_size' : integer
Batch_size of the model
'lr' : float
Learning rate
'solver' : string, must be compatible with TorchDiffEq.odeint_adjoint
Type of ODE solver to be used to solve the NeuralODE</p>
<h2 id="parameters">Parameters</h2>
<p>hparams : An argparse.Namespace object with fields 'train_dataset', 'hidden_nodes', 'batch_size', 'lr', and 'solver'.
Hyperparameters of the model.
'train_dataset' : An instance of the class GeneralModeDataset
A PyTorch dataset of a given general mode approach data to train the NeuralODE with.
'hidden nodes' : list or array of integers
List of number of nodes in the hidden layers of the model.
'batch_size' : integer
Batch_size of the model
'lr' : float
Learning rate
'solver' : string, must be compatible with TorchDiffEq.odeint_adjoint
Type of ODE solver to be used to solve the NeuralODE</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LightningTrainer(pl.LightningModule):
    &#34;&#34;&#34;
    A PyTorch-Lightning LightningModule for training the NeuralODE created by the class AFM_NeuralODE. 

    ...

    Attributes
    ----------
    ODE : An instance of class AFM_NeuralODE
        A NeuralODE model to represent the dynamics between the tip and the sample.
    hparams : An argparse.Namespace object with fields &#39;train_dataset&#39;, &#39;hidden_nodes&#39;, &#39;batch_size&#39;, &#39;lr&#39;, and &#39;solver&#39;.
            Hyperparameters of the model. 
            &#39;train_dataset&#39; : An instance of the class GeneralModeDataset
                A PyTorch dataset of a given general mode approach data to train the NeuralODE with. 
            &#39;hidden nodes&#39; : list or array of integers
                List of number of nodes in the hidden layers of the model.
            &#39;batch_size&#39; : integer
                Batch_size of the model
            &#39;lr&#39; : float
                Learning rate
            &#39;solver&#39; : string, must be compatible with TorchDiffEq.odeint_adjoint
                Type of ODE solver to be used to solve the NeuralODE
    &#34;&#34;&#34;

    #def __init__(self, train_dataset, lr = 0.05, hidden_nodes = [10, 10], batch_size = 1, solver = &#39;dopri5&#39;):
    def __init__(self, hparams, verbose = True):
        &#34;&#34;&#34;
        Parameters
        ----------
        hparams : An argparse.Namespace object with fields &#39;train_dataset&#39;, &#39;hidden_nodes&#39;, &#39;batch_size&#39;, &#39;lr&#39;, and &#39;solver&#39;.
            Hyperparameters of the model. 
            &#39;train_dataset&#39; : An instance of the class GeneralModeDataset
                A PyTorch dataset of a given general mode approach data to train the NeuralODE with. 
            &#39;hidden nodes&#39; : list or array of integers
                List of number of nodes in the hidden layers of the model.
            &#39;batch_size&#39; : integer
                Batch_size of the model
            &#39;lr&#39; : float
                Learning rate
            &#39;solver&#39; : string, must be compatible with TorchDiffEq.odeint_adjoint
                Type of ODE solver to be used to solve the NeuralODE
        &#34;&#34;&#34;
        super(LightningTrainer, self).__init__()
        self.hparams = hparams
        self.train_dataset = GeneralModeDataset.load(self.hparams.train_dataset_path)
        ode_params = self.train_dataset.ode_params
        self.ODE = AFM_NeuralODE(**ode_params, hidden_nodes = self.hparams.hidden_nodes)
        self.batch_size = self.hparams.batch_size
        self.lr = self.hparams.lr
        self.solver = self.hparams.solver
        self._verbose = verbose
        self._fft_loss = self.hparams.fft_loss

    def forward(self, t, x0, d):
        self.ODE.d = d.view(self.batch_size, 1)

        x_pred = odeint(self.ODE, x0, t, method = self.solver, rtol = 1e-6)
        # x_pred has shape = [time, batch_size, 2]. Permute z_pred so that it has shape = [batch_size, time]
        z_pred = x_pred[:,:,1].permute(1,0)

        return z_pred

    def training_step(self, batch, batch_nb):
        t = batch[&#39;time&#39;][0].float()
        x0 = batch[&#39;x0&#39;].float()
        d = batch[&#39;d&#39;].float()

        z_true = batch[&#39;z&#39;].float()
        N_data = z_true.size(1) # length of the validation data
        
        if self._verbose:
            sys.stdout.write(&#39;\r&#39;)
            sys.stdout.write(&#39;Forward-propagating...&#39;)
            sys.stdout.flush()

        z_pred = self.forward(t, x0, d)[:, -N_data:]
        loss_function = nn.MSELoss()
        if self._fft_loss:
            log1pI_pred = self.LogSpectra(z_pred)
            log1pI_true = self.LogSpectra(z_true)

            loss = loss_function(log1pI_pred, log1pI_true)
        else:
            loss = loss_function(z_true, z_pred)

        if self._verbose:
            sys.stdout.write(&#39;\r&#39;)
            sys.stdout.write(&#39;Backward-propagating...&#39;)
            sys.stdout.flush()

        tensorboard_logs = {&#39;train_loss&#39;: loss}
        return {&#39;loss&#39;: loss, &#39;log&#39;: tensorboard_logs}

    def train_dataloader(self):
        return DataLoader(self.train_dataset, batch_size = self.batch_size, shuffle = True)

    def validation_step(self, batch, batch_nb):
        pass

    @staticmethod
    def LogSpectra(z):
        &#34;&#34;&#34;
        Calculates the complex FFT spectra of the given signal z(t), then calculates the intensity. Finally, returns log1p(intensity), where log1p is preferred over log due to its numerical stability.

        Parameters
        ----------
        z : A 1D PyTorch tensor
            1D tensor of real values, corresponding to the time series z(t)

        Returns
        -------
        z_log1pI : A 1d Pytorch tensor
            1D tensor corresponding to the log1p of the Fourier intensity of z(t).
        &#34;&#34;&#34;

        z_fft = torch.rfft(z, 1)
        #z_I = torch.sum(z_fft**2, dim = -1)
        #z_log1pI = torch.norm(z_fft, p = 2, dim = -1)
        #z_log1pI = torch.log1p(z_I)

        return z_fft

    def configure_optimizers(self):
        optim = torch.optim.Adam(self.parameters(), lr = self.lr)
        #sched = torch.optim.lr_scheduler.ReduceLROnPlateau(optim, &#39;min&#39;, 0.5, 100, threshold = 0.05, threshold_mode = &#39;rel&#39;)
        #return [optim], [sched]
        return optim

    def predict_z(self):
        pass

    def predict_force(self, d):
        d = np.array(d)
        d_tensor = torch.from_numpy(d).cuda(non_blocking = True).float().reshape(-1, 1)
        F_pred = self.ODE.Fc(d_tensor).cpu().detach().numpy()
        return F_pred

    def TrainModel(self, max_epochs = 10000, checkpoint_path = &#39;./checkpoints&#39;):
        &#34;&#34;&#34;
        Trains the model using PyTorch_Lightning.trainer. 

        Parameters
        ----------
        model : An instance of PyTorch_Lightning.LightningModule
            Neural network to be trained. 
        max_epochs : integer
            Maximum number of training.
        checkpoint_path : path
            Path to save the checkpointed model during training.
        &#34;&#34;&#34;
        #logger = TensorBoardLogger(save_dir = os.getcwd(), version = self.slurm_job_id, name = &#39;lightning_logs&#39;)
        checkpoint_callback = ModelCheckpoint(filepath = checkpoint_path, save_top_k = 1, verbose = True, monitor = &#39;loss&#39;, mode = &#39;min&#39;)
        trainer = pl.Trainer(gpus = 1, checkpoint_callback = checkpoint_callback, early_stop_callback = None, show_progress_bar = True, max_nb_epochs = max_epochs, log_save_interval = 1)
        trainer.fit(self)

    @classmethod
    def LoadModel(cls, checkpoint_path):
        &#34;&#34;&#34;
        Loads the checkpointed model from checkpoint_path. 

        Parameters
        ----------
        checkpoint_path : path
            Path to load the checkpointed model from.

        Returns
        -------
        loaded_model : An instance of PyTorch_Lightning.LightningModule
            Loaded neural network.
        &#34;&#34;&#34;
        return cls.load_from_checkpoint(checkpoint_path)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>pytorch_lightning.core.lightning.LightningModule</li>
<li>abc.ABC</li>
<li>pytorch_lightning.core.grads.GradInformation</li>
<li>pytorch_lightning.core.saving.ModelIO</li>
<li>pytorch_lightning.core.hooks.ModelHooks</li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="cp_detection.NeuralODE.LightningTrainer.LoadModel"><code class="name flex">
<span>def <span class="ident">LoadModel</span></span>(<span>checkpoint_path)</span>
</code></dt>
<dd>
<section class="desc"><p>Loads the checkpointed model from checkpoint_path. </p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>checkpoint_path</code></strong> :&ensp;<code>path</code></dt>
<dd>Path to load the checkpointed model from.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>loaded_model</code></strong> :&ensp;<code>An</code> <code>instance</code> of <code>PyTorch_Lightning.LightningModule</code></dt>
<dd>Loaded neural network.</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def LoadModel(cls, checkpoint_path):
    &#34;&#34;&#34;
    Loads the checkpointed model from checkpoint_path. 

    Parameters
    ----------
    checkpoint_path : path
        Path to load the checkpointed model from.

    Returns
    -------
    loaded_model : An instance of PyTorch_Lightning.LightningModule
        Loaded neural network.
    &#34;&#34;&#34;
    return cls.load_from_checkpoint(checkpoint_path)</code></pre>
</details>
</dd>
<dt id="cp_detection.NeuralODE.LightningTrainer.LogSpectra"><code class="name flex">
<span>def <span class="ident">LogSpectra</span></span>(<span>z)</span>
</code></dt>
<dd>
<section class="desc"><p>Calculates the complex FFT spectra of the given signal z(t), then calculates the intensity. Finally, returns log1p(intensity), where log1p is preferred over log due to its numerical stability.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>z</code></strong> :&ensp;<code>A</code> <code>1D</code> <code>PyTorch</code> <code>tensor</code></dt>
<dd>1D tensor of real values, corresponding to the time series z(t)</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>z_log1pI</code></strong> :&ensp;<code>A</code> <code>1d</code> <code>Pytorch</code> <code>tensor</code></dt>
<dd>1D tensor corresponding to the log1p of the Fourier intensity of z(t).</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def LogSpectra(z):
    &#34;&#34;&#34;
    Calculates the complex FFT spectra of the given signal z(t), then calculates the intensity. Finally, returns log1p(intensity), where log1p is preferred over log due to its numerical stability.

    Parameters
    ----------
    z : A 1D PyTorch tensor
        1D tensor of real values, corresponding to the time series z(t)

    Returns
    -------
    z_log1pI : A 1d Pytorch tensor
        1D tensor corresponding to the log1p of the Fourier intensity of z(t).
    &#34;&#34;&#34;

    z_fft = torch.rfft(z, 1)
    #z_I = torch.sum(z_fft**2, dim = -1)
    #z_log1pI = torch.norm(z_fft, p = 2, dim = -1)
    #z_log1pI = torch.log1p(z_I)

    return z_fft</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="cp_detection.NeuralODE.LightningTrainer.TrainModel"><code class="name flex">
<span>def <span class="ident">TrainModel</span></span>(<span>self, max_epochs=10000, checkpoint_path='./checkpoints')</span>
</code></dt>
<dd>
<section class="desc"><p>Trains the model using PyTorch_Lightning.trainer. </p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>An</code> <code>instance</code> of <code>PyTorch_Lightning.LightningModule</code></dt>
<dd>Neural network to be trained.</dd>
<dt><strong><code>max_epochs</code></strong> :&ensp;<code>integer</code></dt>
<dd>Maximum number of training.</dd>
<dt><strong><code>checkpoint_path</code></strong> :&ensp;<code>path</code></dt>
<dd>Path to save the checkpointed model during training.</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def TrainModel(self, max_epochs = 10000, checkpoint_path = &#39;./checkpoints&#39;):
    &#34;&#34;&#34;
    Trains the model using PyTorch_Lightning.trainer. 

    Parameters
    ----------
    model : An instance of PyTorch_Lightning.LightningModule
        Neural network to be trained. 
    max_epochs : integer
        Maximum number of training.
    checkpoint_path : path
        Path to save the checkpointed model during training.
    &#34;&#34;&#34;
    #logger = TensorBoardLogger(save_dir = os.getcwd(), version = self.slurm_job_id, name = &#39;lightning_logs&#39;)
    checkpoint_callback = ModelCheckpoint(filepath = checkpoint_path, save_top_k = 1, verbose = True, monitor = &#39;loss&#39;, mode = &#39;min&#39;)
    trainer = pl.Trainer(gpus = 1, checkpoint_callback = checkpoint_callback, early_stop_callback = None, show_progress_bar = True, max_nb_epochs = max_epochs, log_save_interval = 1)
    trainer.fit(self)</code></pre>
</details>
</dd>
<dt id="cp_detection.NeuralODE.LightningTrainer.configure_optimizers"><code class="name flex">
<span>def <span class="ident">configure_optimizers</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Choose what optimizers and learning-rate schedulers to use in your optimization.
Normally you'd need one. But in the case of GANs or similar you might have multiple.</p>
<p>If you don't define this method Lightning will automatically use Adam(lr=1e-3)</p>
<p>Return: any of these 3 options:
- Single optimizer
- List or Tuple - List of optimizers
- Two lists - The first list has multiple optimizers, the second a list of LR schedulers</p>
<h2 id="examples">Examples</h2>
<p>.. code-block:: python</p>
<pre><code># most cases (default if not defined)
def configure_optimizers(self):
    opt = Adam(self.parameters(), lr=1e-3)
    return opt

# multiple optimizer case (eg: GAN)
def configure_optimizers(self):
    generator_opt = Adam(self.model_gen.parameters(), lr=0.01)
    disriminator_opt = Adam(self.model_disc.parameters(), lr=0.02)
    return generator_opt, disriminator_opt

# example with learning_rate schedulers
def configure_optimizers(self):
    generator_opt = Adam(self.model_gen.parameters(), lr=0.01)
    disriminator_opt = Adam(self.model_disc.parameters(), lr=0.02)
    discriminator_sched = CosineAnnealing(discriminator_opt, T_max=10)
    return [generator_opt, disriminator_opt], [discriminator_sched]

# example with step-based learning_rate schedulers
def configure_optimizers(self):
    gen_opt = Adam(self.model_gen.parameters(), lr=0.01)
    dis_opt = Adam(self.model_disc.parameters(), lr=0.02)
    gen_sched = {'scheduler': ExponentialLR(gen_opt, 0.99),
                 'interval': 'step'}  # called after each training step
    dis_sched = CosineAnnealing(discriminator_opt, T_max=10) # called every epoch
    return [gen_opt, dis_opt], [gen_sched, dis_sched]
</code></pre>
<p>Some things to know</p>
<pre><code>- Lightning calls ``.backward()`` and ``.step()`` on each optimizer
and learning rate scheduler as needed.

- If you use 16-bit precision (``precision=16``), Lightning will automatically
handle the optimizers for you.

- If you use multiple optimizers, training_step will have an additional
``optimizer_idx`` parameter.

- If you use LBFGS lightning handles the closure function automatically for you

- If you use multiple optimizers, gradients will be calculated only
for the parameters of current optimizer at each training step.

- If you need to control how often those optimizers step or override the
default .step() schedule, override the `optimizer_step` hook.

- If you only want to call a learning rate scheduler every `x` step or epoch,
you can input this as 'frequency' key: dict(scheduler=lr_scheduler,
                                            interval='step' or 'epoch', frequency=x)
</code></pre></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def configure_optimizers(self):
    optim = torch.optim.Adam(self.parameters(), lr = self.lr)
    #sched = torch.optim.lr_scheduler.ReduceLROnPlateau(optim, &#39;min&#39;, 0.5, 100, threshold = 0.05, threshold_mode = &#39;rel&#39;)
    #return [optim], [sched]
    return optim</code></pre>
</details>
</dd>
<dt id="cp_detection.NeuralODE.LightningTrainer.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, t, x0, d)</span>
</code></dt>
<dd>
<section class="desc"><p>Same as torch.nn.Module.forward(), however in Lightning you want this to define
the
operations you want to use for prediction (ie: on a server or as a feature extractor).</p>
<p>Normally you'd call self.forward() from your training_step() method.
This makes it easy to write a complex system for training with the outputs
you'd want in a prediction setting.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>tensor</code></dt>
<dd>Whatever
you decide to define in the forward method</dd>
</dl>
<h2 id="return">Return</h2>
<p>Predicted output</p>
<h2 id="examples">Examples</h2>
<p>.. code-block:: python</p>
<pre><code># example if we were using this model as a feature extractor
def forward(self, x):
    feature_maps = self.convnet(x)
    return feature_maps

def training_step(self, batch, batch_idx):
    x, y = batch
    feature_maps = self.forward(x)
    logits = self.classifier(feature_maps)

    # ...
    return loss

# splitting it this way allows model to be used a feature extractor
model = MyModelAbove()

inputs = server.get_request()
results = model(inputs)
server.write_results(results)

# -------------
# This is in stark contrast to torch.nn.Module where normally you would have this:
def forward(self, batch):
    x, y = batch
    feature_maps = self.convnet(x)
    logits = self.classifier(feature_maps)
    return logits
</code></pre></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, t, x0, d):
    self.ODE.d = d.view(self.batch_size, 1)

    x_pred = odeint(self.ODE, x0, t, method = self.solver, rtol = 1e-6)
    # x_pred has shape = [time, batch_size, 2]. Permute z_pred so that it has shape = [batch_size, time]
    z_pred = x_pred[:,:,1].permute(1,0)

    return z_pred</code></pre>
</details>
</dd>
<dt id="cp_detection.NeuralODE.LightningTrainer.predict_force"><code class="name flex">
<span>def <span class="ident">predict_force</span></span>(<span>self, d)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict_force(self, d):
    d = np.array(d)
    d_tensor = torch.from_numpy(d).cuda(non_blocking = True).float().reshape(-1, 1)
    F_pred = self.ODE.Fc(d_tensor).cpu().detach().numpy()
    return F_pred</code></pre>
</details>
</dd>
<dt id="cp_detection.NeuralODE.LightningTrainer.predict_z"><code class="name flex">
<span>def <span class="ident">predict_z</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict_z(self):
    pass</code></pre>
</details>
</dd>
<dt id="cp_detection.NeuralODE.LightningTrainer.train_dataloader"><code class="name flex">
<span>def <span class="ident">train_dataloader</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Implement a PyTorch DataLoader</p>
<h2 id="return">Return</h2>
<p>PyTorch DataLoader
Return a dataloader. It will not be called every epoch unless you set
<code>Trainer(reload_dataloaders_every_epoch=True)</code>.</p>
<p>It's recommended that all data downloads and preparation happen in prepare_data().</p>
<div class="admonition note">
<p class="admonition-title">Note:&ensp;Lightning adds the correct sampler for distributed and arbitrary hardware.</p>
<p>No need to set yourself.</p>
<ul>
<li>.fit()</li>
<li>&hellip;</li>
<li>prepare_data()</li>
<li>train_dataloader</li>
</ul>
</div>
<h2 id="example">Example</h2>
<p>.. code-block:: python</p>
<pre><code>def train_dataloader(self):
    transform = transforms.Compose([transforms.ToTensor(),
                                    transforms.Normalize((0.5,), (1.0,))])
    dataset = MNIST(root='/path/to/mnist/', train=True, transform=transform,
                    download=True)
    loader = torch.utils.data.DataLoader(
        dataset=dataset,
        batch_size=self.hparams.batch_size,
        shuffle=True
    )
    return loader
</code></pre></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train_dataloader(self):
    return DataLoader(self.train_dataset, batch_size = self.batch_size, shuffle = True)</code></pre>
</details>
</dd>
<dt id="cp_detection.NeuralODE.LightningTrainer.training_step"><code class="name flex">
<span>def <span class="ident">training_step</span></span>(<span>self, batch, batch_nb)</span>
</code></dt>
<dd>
<section class="desc"><p>return loss, dict with metrics for tqdm</p>
<h2 id="args">Args</h2>
<dl>
<dt>batch (torch.nn.Tensor | (Tensor, Tensor) | [Tensor, Tensor]): The output of your</dt>
<dt>dataloader. A tensor, tuple or list</dt>
<dt><strong><code>batch_idx</code></strong> :&ensp;<code>int</code></dt>
<dd>Integer displaying index of this batch</dd>
<dt><strong><code>optimizer_idx</code></strong> :&ensp;<code>int</code></dt>
<dd>If using multiple optimizers, this argument will also be present.</dd>
</dl>
<p>hiddens(:<code>Tensor &lt;https://pytorch.org/docs/stable/tensors.html&gt;</code>_):
Passed in if truncated_bptt_steps &gt; 0.</p>
<h2 id="return">Return</h2>
<p>dict with loss key and optional log, progress keys
if implementing training_step, return whatever you need in that step:</p>
<pre><code>- loss -&gt; tensor scalar [REQUIRED]
- progress_bar -&gt; Dict for progress bar display. Must have only tensors
- log -&gt; Dict of metrics to add to logger. Must have only tensors (no images, etc)
</code></pre>
<p>In this step you'd normally do the forward pass and calculate the loss for a batch.
You can also do fancier things like multiple forward passes or something model specific.</p>
<h2 id="examples">Examples</h2>
<p>.. code-block:: python</p>
<pre><code>def training_step(self, batch, batch_idx):
    x, y, z = batch

    # implement your own
    out = self.forward(x)
    loss = self.loss(out, x)

    logger_logs = {'training_loss': loss} # optional (MUST ALL BE TENSORS)

    # if using TestTubeLogger or TensorBoardLogger you can nest scalars
    logger_logs = {'losses': logger_logs} # optional (MUST ALL BE TENSORS)

    output = {
        'loss': loss, # required
        'progress_bar': {'training_loss': loss}, # optional (MUST ALL BE TENSORS)
        'log': logger_logs
    }

    # return a dict
    return output
</code></pre>
<p>If you define multiple optimizers, this step will be called with an additional
<code>optimizer_idx</code> param.</p>
<p>.. code-block:: python</p>
<pre><code># Multiple optimizers (ie: GANs)
def training_step(self, batch, batch_idx, optimizer_idx):
    if optimizer_idx == 0:
        # do training_step with encoder
    if optimizer_idx == 1:
        # do training_step with decoder
</code></pre>
<p>If you add truncated back propagation through time you will also get an additional
argument with the hidden states of the previous step.</p>
<p>.. code-block:: python</p>
<pre><code># Truncated back-propagation through time
def training_step(self, batch, batch_idx, hiddens):
    # hiddens are the hiddens from the previous truncated backprop step
    ...
    out, hiddens = self.lstm(data, hiddens)
    ...

    return {
        "loss": ...,
        "hiddens": hiddens  # remember to detach() this
    }
</code></pre>
<p>You can also return a -1 instead of a dict to stop the current loop. This is useful
if you want to break out of the current training epoch early.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def training_step(self, batch, batch_nb):
    t = batch[&#39;time&#39;][0].float()
    x0 = batch[&#39;x0&#39;].float()
    d = batch[&#39;d&#39;].float()

    z_true = batch[&#39;z&#39;].float()
    N_data = z_true.size(1) # length of the validation data
    
    if self._verbose:
        sys.stdout.write(&#39;\r&#39;)
        sys.stdout.write(&#39;Forward-propagating...&#39;)
        sys.stdout.flush()

    z_pred = self.forward(t, x0, d)[:, -N_data:]
    loss_function = nn.MSELoss()
    if self._fft_loss:
        log1pI_pred = self.LogSpectra(z_pred)
        log1pI_true = self.LogSpectra(z_true)

        loss = loss_function(log1pI_pred, log1pI_true)
    else:
        loss = loss_function(z_true, z_pred)

    if self._verbose:
        sys.stdout.write(&#39;\r&#39;)
        sys.stdout.write(&#39;Backward-propagating...&#39;)
        sys.stdout.flush()

    tensorboard_logs = {&#39;train_loss&#39;: loss}
    return {&#39;loss&#39;: loss, &#39;log&#39;: tensorboard_logs}</code></pre>
</details>
</dd>
<dt id="cp_detection.NeuralODE.LightningTrainer.validation_step"><code class="name flex">
<span>def <span class="ident">validation_step</span></span>(<span>self, batch, batch_nb)</span>
</code></dt>
<dd>
<section class="desc"><p>Operate on a single batch of data from the validation set
In this step you'd might generate examples or calculate anything of interest like accuracy.</p>
<p>.. code-block:: python</p>
<pre><code># the pseudocode for these calls
val_outs = []
for val_batch in val_data:
    out = validation_step(train_batch)
    val_outs.append(out
    validation_epoch_end(val_outs)
</code></pre>
<h2 id="args">Args</h2>
<dl>
<dt>batch (torch.nn.Tensor | (Tensor, Tensor) | [Tensor, Tensor]): The output of your</dt>
<dt>dataloader. A tensor, tuple or list</dt>
<dt><strong><code>batch_idx</code></strong> :&ensp;<code>int</code></dt>
<dd>The index of this batch</dd>
<dt><strong><code>dataloader_idx</code></strong> :&ensp;<code>int</code></dt>
<dd>The index of the dataloader that produced this batch
(only if multiple val datasets used)</dd>
</dl>
<h2 id="return">Return</h2>
<p>Dict or OrderedDict - passed to validation_epoch_end.
If you defined validation_step_end it will go to that first.
.. code-block:: python</p>
<pre><code># pseudocode of order
out = validation_step()
if defined('validation_step_end'):
    out = validation_step_end(out)
out = validation_epoch_end(out)
</code></pre>
<p>.. code-block:: python</p>
<pre><code># if you have one val dataloader:
def validation_step(self, batch, batch_idx)

# if you have multiple val dataloaders:
def validation_step(self, batch, batch_idx, dataloader_idx)
</code></pre>
<h2 id="examples">Examples</h2>
<p>.. code-block:: python</p>
<pre><code># CASE 1: A single validation dataset
def validation_step(self, batch, batch_idx):
    x, y = batch

    # implement your own
    out = self.forward(x)
    loss = self.loss(out, y)

    # log 6 example images
    # or generated text... or whatever
    sample_imgs = x[:6]
    grid = torchvision.utils.make_grid(sample_imgs)
    self.logger.experiment.add_image('example_images', grid, 0)

    # calculate acc
    labels_hat = torch.argmax(out, dim=1)
    val_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0)

    # all optional...
    # return whatever you need for the collation function validation_end
    output = OrderedDict({
        'val_loss': loss_val,
        'val_acc': torch.tensor(val_acc), # everything must be a tensor
    })

    # return an optional dict
    return output
</code></pre>
<p>If you pass in multiple val datasets, validation_step will have an additional argument.</p>
<p>.. code-block:: python</p>
<div class="admonition note">
<p class="admonition-title">Note:&ensp;If you don't need to validate you don't need to implement this method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note:&ensp;When the validation_step is called, the model has been put in eval mode</p>
<p>and PyTorch gradients have been disabled. At the end of validation,
the model goes back to training mode and gradients are enabled.</p>
</div></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def validation_step(self, batch, batch_nb):
    pass</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="cp_detection" href="index.html">cp_detection</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="cp_detection.NeuralODE.AFM_NeuralODE" href="#cp_detection.NeuralODE.AFM_NeuralODE">AFM_NeuralODE</a></code></h4>
<ul class="">
<li><code><a title="cp_detection.NeuralODE.AFM_NeuralODE.forward" href="#cp_detection.NeuralODE.AFM_NeuralODE.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="cp_detection.NeuralODE.F_cons" href="#cp_detection.NeuralODE.F_cons">F_cons</a></code></h4>
<ul class="">
<li><code><a title="cp_detection.NeuralODE.F_cons.forward" href="#cp_detection.NeuralODE.F_cons.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="cp_detection.NeuralODE.GeneralModeDataset" href="#cp_detection.NeuralODE.GeneralModeDataset">GeneralModeDataset</a></code></h4>
<ul class="">
<li><code><a title="cp_detection.NeuralODE.GeneralModeDataset.AddNoise" href="#cp_detection.NeuralODE.GeneralModeDataset.AddNoise">AddNoise</a></code></li>
<li><code><a title="cp_detection.NeuralODE.GeneralModeDataset.PlotData" href="#cp_detection.NeuralODE.GeneralModeDataset.PlotData">PlotData</a></code></li>
<li><code><a title="cp_detection.NeuralODE.GeneralModeDataset.load" href="#cp_detection.NeuralODE.GeneralModeDataset.load">load</a></code></li>
<li><code><a title="cp_detection.NeuralODE.GeneralModeDataset.save" href="#cp_detection.NeuralODE.GeneralModeDataset.save">save</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="cp_detection.NeuralODE.LightningTrainer" href="#cp_detection.NeuralODE.LightningTrainer">LightningTrainer</a></code></h4>
<ul class="">
<li><code><a title="cp_detection.NeuralODE.LightningTrainer.LoadModel" href="#cp_detection.NeuralODE.LightningTrainer.LoadModel">LoadModel</a></code></li>
<li><code><a title="cp_detection.NeuralODE.LightningTrainer.LogSpectra" href="#cp_detection.NeuralODE.LightningTrainer.LogSpectra">LogSpectra</a></code></li>
<li><code><a title="cp_detection.NeuralODE.LightningTrainer.TrainModel" href="#cp_detection.NeuralODE.LightningTrainer.TrainModel">TrainModel</a></code></li>
<li><code><a title="cp_detection.NeuralODE.LightningTrainer.configure_optimizers" href="#cp_detection.NeuralODE.LightningTrainer.configure_optimizers">configure_optimizers</a></code></li>
<li><code><a title="cp_detection.NeuralODE.LightningTrainer.forward" href="#cp_detection.NeuralODE.LightningTrainer.forward">forward</a></code></li>
<li><code><a title="cp_detection.NeuralODE.LightningTrainer.predict_force" href="#cp_detection.NeuralODE.LightningTrainer.predict_force">predict_force</a></code></li>
<li><code><a title="cp_detection.NeuralODE.LightningTrainer.predict_z" href="#cp_detection.NeuralODE.LightningTrainer.predict_z">predict_z</a></code></li>
<li><code><a title="cp_detection.NeuralODE.LightningTrainer.train_dataloader" href="#cp_detection.NeuralODE.LightningTrainer.train_dataloader">train_dataloader</a></code></li>
<li><code><a title="cp_detection.NeuralODE.LightningTrainer.training_step" href="#cp_detection.NeuralODE.LightningTrainer.training_step">training_step</a></code></li>
<li><code><a title="cp_detection.NeuralODE.LightningTrainer.validation_step" href="#cp_detection.NeuralODE.LightningTrainer.validation_step">validation_step</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.7.2</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>